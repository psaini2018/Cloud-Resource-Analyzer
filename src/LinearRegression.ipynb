{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for Modeling Cloud Resource Usage Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from pandas import read_csv, datetime\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from dateutil.relativedelta import relativedelta \n",
    "from scipy.optimize import minimize              \n",
    "import statsmodels.formula.api as smf            \n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from itertools import product                    \n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings                                \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'/home/julian/CMPE295/DataSet/rnd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = base_path+'2013-7/'                     # use your path\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join \n",
    "df_from_each_file = (pd.read_csv(f, sep = ';\\t').assign(VM=os.path.basename(f).split('.')[0]) for f in all_files)\n",
    "concatenated_df   = pd.concat(df_from_each_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = base_path+'2013-8/'                     \n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))     \n",
    "df_from_each_file = (pd.read_csv(f, sep = ';\\t').assign(VM=os.path.basename(f).split('.')[0]) for f in all_files)\n",
    "concatenated_df8   = pd.concat(df_from_each_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 293. MiB for an array with shape (9, 4268723) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-98de636b2ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_from_each_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m';\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconcatenated_df9\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_from_each_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             )\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2016\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[0;32m-> 2018\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2019\u001b[0m             )\n\u001b[1;32m   2020\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[0;34m(self, to_concat, placement)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \"\"\"\n\u001b[1;32m    358\u001b[0m         values = self._concatenator(\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         )\n\u001b[1;32m    361\u001b[0m         return self.make_block_same_class(\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 293. MiB for an array with shape (9, 4268723) and data type float64"
     ]
    }
   ],
   "source": [
    "path = base_path+'2013-9/'                   \n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))  \n",
    "df_from_each_file = (pd.read_csv(f, sep = ';\\t').assign(VM=os.path.basename(f).split('.')[0]) for f in all_files)\n",
    "concatenated_df9   = pd.concat(df_from_each_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdat = concatenated_df.append(concatenated_df8)\n",
    "newerdat = newdat.append(concatenated_df9)\n",
    "concatenated_df = newerdat\n",
    "concatenated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and converting pandas into a timeseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df['Timestamp'] = pd.to_datetime(concatenated_df['Timestamp [ms]'], unit = 's')\n",
    "concatenated_df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# Date Feature Engineering\n",
    "concatenated_df['weekday'] = concatenated_df['Timestamp'].dt.dayofweek\n",
    "concatenated_df['weekend'] = ((concatenated_df.weekday) // 5 == 1).astype(float)\n",
    "concatenated_df['month']=concatenated_df.Timestamp.dt.month \n",
    "concatenated_df['day']=concatenated_df.Timestamp.dt.day\n",
    "concatenated_df.set_index('Timestamp',inplace=True)\n",
    "\n",
    "# Other Feature Engineering\n",
    "concatenated_df[\"CPU usage prev\"] = concatenated_df['CPU usage [%]'].shift(1)\n",
    "concatenated_df[\"CPU_diff\"] = concatenated_df['CPU usage [%]'] - concatenated_df[\"CPU usage prev\"]\n",
    "concatenated_df[\"received_prev\"] = concatenated_df['Network received throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"received_diff\"] = concatenated_df['Network received throughput [KB/s]']- concatenated_df[\"received_prev\"]\n",
    "concatenated_df[\"transmitted_prev\"] = concatenated_df['Network transmitted throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"transmitted_diff\"] = concatenated_df['Network transmitted throughput [KB/s]']- concatenated_df[\"transmitted_prev\"]\n",
    "\n",
    "concatenated_df[\"write_prev\"] = concatenated_df['Disk write throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"write_diff\"] = concatenated_df['Disk write throughput [KB/s]']- concatenated_df[\"write_prev\"]\n",
    "concatenated_df[\"read_prev\"] = concatenated_df['Disk read throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"read_diff\"] = concatenated_df['Disk read throughput [KB/s]']- concatenated_df[\"read_prev\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values using forward propagating function from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = concatenated_df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new data frame: resampled & aggregated over each hour for all VMs.\n",
    "This allows to get a broader picture of the network's activity. \n",
    "Examine autocorrelations of hourly transmitted, received, and CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlydat = concatenated_df.resample('H').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hourly resampled means\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.plotting.autocorrelation_plot(hourlydat['CPU usage [MHZ]']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is CPU Capacity Ever Met?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overprovision = pd.DataFrame(hourlydat['CPU usage [MHZ]'])\n",
    "overprovision['CPU capacity provisioned'] = pd.DataFrame(hourlydat['CPU capacity provisioned [MHZ]'])\n",
    "#overprovision.to_csv(\"overprovision.csv\", sep = \",\")\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "overprovision.plot(figsize = (12,10),linewidth=2.5,fontsize=20)\n",
    "plt.title('Is CPU Capacity Ever Met?',fontsize=22)\n",
    "plt.ylabel((r'CPU [MHz]  $e^{7}$'), fontsize=20);\n",
    "plt.xlabel('Date', fontsize=20);\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks( fontsize = 15)\n",
    "plt.legend(loc=\"best\", fontsize =14)\n",
    "plt.ticklabel_format(axis = 'y', style = 'sci', scilimits = (1,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "    \n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.tick_params(labelsize=20)\n",
    "    plt.xlabel(\"Features\", fontsize = 20) \n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm  \n",
    "from statsmodels.tsa.stattools import acf  \n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(hourlydat['CPU usage [MHZ]'].copy())\n",
    "data.columns = [\"y\"]\n",
    "\n",
    "# Adding the lag of the target variable from 3 steps (hours) up to 24\n",
    "for i in range(3, 25):\n",
    "    data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "\n",
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for time-series cross-validation set 5 folds \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where keys are unique categories\n",
    "    and values are means\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())\n",
    "\n",
    "def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n",
    "    \"\"\"\n",
    "        series: pd.DataFrame\n",
    "            dataframe with timeseries\n",
    "        lag_start: int\n",
    "            initial step back in time to slice target variable \n",
    "            example - lag_start = 1 means that the model \n",
    "                      will see yesterday's values to predict today\n",
    "        lag_end: int\n",
    "            final step back in time to slice target variable\n",
    "            example - lag_end = 4 means that the model \n",
    "                      will see up to 4 days back in time to predict today\n",
    "        test_size: float\n",
    "            size of the test dataset after train/test split as percentage of dataset\n",
    "        target_encoding: boolean\n",
    "            if True - add target averages to the dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    # copy of the initial dataset\n",
    "    data = pd.DataFrame(series.copy())\n",
    "    data.columns = [\"y\"]\n",
    "    \n",
    "    # lags of series\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "    \n",
    "    # datetime features\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    # other features\n",
    "    data['network received'] = hourlydat[['Network received throughput [KB/s]']]\n",
    "    data['network transmitted'] = hourlydat[['Network transmitted throughput [KB/s]']]\n",
    "    data['disk read'] = hourlydat[['Disk read throughput [KB/s]']]\n",
    "    data['disk write'] = hourlydat[['Disk write throughput [KB/s]']]\n",
    "    data['cpu diff'] = hourlydat[['CPU_diff']]\n",
    "    data['received_prev'] = hourlydat[['received_prev']]\n",
    "    data['core'] = hourlydat[['CPU cores']]\n",
    "\n",
    "    if target_encoding:\n",
    "        # calculate averages on train set only\n",
    "        test_index = int(len(data.dropna())*(1-test_size))\n",
    "        data['weekday_average'] = list(map(\n",
    "            code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday))\n",
    "        data[\"hour_average\"] = list(map(\n",
    "            code_mean(data[:test_index], 'hour', \"y\").get, data.hour))\n",
    "        \n",
    "        # drop encoded variables \n",
    "        data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "    \n",
    "    # train-test split\n",
    "    y = data.dropna().y\n",
    "    X = data.dropna().drop(['y'], axis=1)\n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    timeseries_train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve 30% of data for testing\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "prepareData(hourlydat[['CPU usage [MHZ]']], lag_start=3, lag_end=10, test_size=0.3, target_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"orange\", label=\"Prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values,label=\"Actual Usage\", linewidth=2.0)\n",
    "    #plt.plot(capprv.values, color='red',label = \"Actual Provision\")\n",
    "\n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"Upper/Lower Bounds\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        plt.tick_params(labelsize=20)\n",
    "        plt.ylabel(\"CPU usage [MHZ]\", fontsize = 20)\n",
    "        plt.xlabel(\"Time\", fontsize = 20) \n",
    "        plt.yticks([2000000,6000000,12000000])# customized for graph readability\n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean Absolute Error: {0:.2f}%\".format(error),  fontsize = 20)\n",
    "    plt.legend(loc=\"best\", fontsize = 15)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModelResultsScaler(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"orange\", label=\"Prediction\", linewidth=4.0)\n",
    "    plt.plot(y_test.values,label=\"Actual Usage\", linewidth=2.0)\n",
    "    \n",
    "    ## if you want to see what was actually provisioned, use the next line\n",
    "    \n",
    "    #plt.plot(capprv.values, color='red',label = \"Actual Provision\")\n",
    "    \n",
    "    plt.ylabel(\"CPU usage [MHZ]\", fontsize = 20)\n",
    "    plt.xlabel(\"Time\", fontsize = 20) \n",
    "    #plt.yticks([2000000,6000000,12000000, 50000000]) # customized for graph readability \n",
    "    plt.tick_params(labelsize=20)\n",
    "    plt.ylabel('CPU [MHz]  $e^{7}$', fontsize=20);\n",
    "    plt.xlabel(\"Time\", fontsize = 20) \n",
    "   \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"b--\", label=\"Upper/Lower Bounds\", alpha=0.5, linewidth=1)\n",
    "        plt.plot(upper, \"b--\", alpha=0.5, linewidth=1)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean Absolute Error {0:.2f}%\".format(error), fontsize = 20)\n",
    "    plt.legend(loc=\"best\", fontsize =15)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResultsScaler(lr, X_train=X_train_scaled, X_test=X_test_scaled, \n",
    "                 plot_intervals=False, plot_anomalies=False)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = '../../models/linear_regression.model'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
