{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for Modeling Cloud Resource Usage Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from pandas import read_csv, datetime\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from dateutil.relativedelta import relativedelta \n",
    "from scipy.optimize import minimize              \n",
    "import statsmodels.formula.api as smf            \n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from itertools import product                    \n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings                                \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'../'\n",
    "data_path = '/home/julian/dataset/'\n",
    "model_path = base_path+'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_path+'2013-7/'                     # use your path\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join \n",
    "df_from_each_file = (pd.read_csv(f, sep = ';\\t').assign(VM=os.path.basename(f).split('.')[0]) for f in all_files)\n",
    "concatenated_df   = pd.concat(df_from_each_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_path+'2013-8/'                     \n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))     \n",
    "df_from_each_file = (pd.read_csv(f, sep = ';\\t').assign(VM=os.path.basename(f).split('.')[0]) for f in all_files)\n",
    "concatenated_df8   = pd.concat(df_from_each_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_path+'2013-9/'                   \n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))  \n",
    "df_from_each_file = (pd.read_csv(f, sep = ';\\t').assign(VM=os.path.basename(f).split('.')[0]) for f in all_files)\n",
    "concatenated_df9   = pd.concat(df_from_each_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12496728, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdat = concatenated_df.append(concatenated_df8)\n",
    "newerdat = newdat.append(concatenated_df9)\n",
    "concatenated_df = newerdat\n",
    "concatenated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp [ms]</th>\n",
       "      <th>CPU cores</th>\n",
       "      <th>CPU capacity provisioned [MHZ]</th>\n",
       "      <th>CPU usage [MHZ]</th>\n",
       "      <th>CPU usage [%]</th>\n",
       "      <th>Memory capacity provisioned [KB]</th>\n",
       "      <th>Memory usage [KB]</th>\n",
       "      <th>Disk read throughput [KB/s]</th>\n",
       "      <th>Disk write throughput [KB/s]</th>\n",
       "      <th>Network received throughput [KB/s]</th>\n",
       "      <th>Network transmitted throughput [KB/s]</th>\n",
       "      <th>VM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372629804</td>\n",
       "      <td>2</td>\n",
       "      <td>5851.9989</td>\n",
       "      <td>87.779984</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>8218624.0</td>\n",
       "      <td>1.034593e+06</td>\n",
       "      <td>160.866667</td>\n",
       "      <td>21.733333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372630104</td>\n",
       "      <td>2</td>\n",
       "      <td>5851.9989</td>\n",
       "      <td>29.259995</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8218624.0</td>\n",
       "      <td>4.585755e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372630404</td>\n",
       "      <td>2</td>\n",
       "      <td>5851.9989</td>\n",
       "      <td>27.309328</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>8218624.0</td>\n",
       "      <td>1.845480e+05</td>\n",
       "      <td>32.066667</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372630704</td>\n",
       "      <td>2</td>\n",
       "      <td>5851.9989</td>\n",
       "      <td>23.407996</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8218624.0</td>\n",
       "      <td>7.829227e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372631004</td>\n",
       "      <td>2</td>\n",
       "      <td>5851.9989</td>\n",
       "      <td>19.506663</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8218624.0</td>\n",
       "      <td>1.677720e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp [ms]  CPU cores  CPU capacity provisioned [MHZ]  CPU usage [MHZ]  \\\n",
       "0      1372629804          2                       5851.9989        87.779984   \n",
       "1      1372630104          2                       5851.9989        29.259995   \n",
       "2      1372630404          2                       5851.9989        27.309328   \n",
       "3      1372630704          2                       5851.9989        23.407996   \n",
       "4      1372631004          2                       5851.9989        19.506663   \n",
       "\n",
       "   CPU usage [%]  Memory capacity provisioned [KB]  Memory usage [KB]  \\\n",
       "0       1.500000                         8218624.0       1.034593e+06   \n",
       "1       0.500000                         8218624.0       4.585755e+05   \n",
       "2       0.466667                         8218624.0       1.845480e+05   \n",
       "3       0.400000                         8218624.0       7.829227e+04   \n",
       "4       0.333333                         8218624.0       1.677720e+05   \n",
       "\n",
       "   Disk read throughput [KB/s]  Disk write throughput [KB/s]  \\\n",
       "0                   160.866667                     21.733333   \n",
       "1                     0.000000                      2.333333   \n",
       "2                    32.066667                      4.200000   \n",
       "3                     0.000000                      0.866667   \n",
       "4                     0.000000                      0.200000   \n",
       "\n",
       "   Network received throughput [KB/s]  Network transmitted throughput [KB/s]  \\\n",
       "0                            0.266667                               1.466667   \n",
       "1                            0.200000                               1.000000   \n",
       "2                            0.133333                               1.066667   \n",
       "3                            0.066667                               1.000000   \n",
       "4                            0.133333                               1.000000   \n",
       "\n",
       "  VM  \n",
       "0  1  \n",
       "1  1  \n",
       "2  1  \n",
       "3  1  \n",
       "4  1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and converting pandas into a timeseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df['Timestamp'] = pd.to_datetime(concatenated_df['Timestamp [ms]'], unit = 's')\n",
    "concatenated_df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# Date Feature Engineering\n",
    "concatenated_df['weekday'] = concatenated_df['Timestamp'].dt.dayofweek\n",
    "concatenated_df['weekend'] = ((concatenated_df.weekday) // 5 == 1).astype(float)\n",
    "concatenated_df['month']=concatenated_df.Timestamp.dt.month \n",
    "concatenated_df['day']=concatenated_df.Timestamp.dt.day\n",
    "concatenated_df.set_index('Timestamp',inplace=True)\n",
    "\n",
    "# Other Feature Engineering\n",
    "concatenated_df[\"CPU usage prev\"] = concatenated_df['CPU usage [%]'].shift(1)\n",
    "concatenated_df[\"CPU_diff\"] = concatenated_df['CPU usage [%]'] - concatenated_df[\"CPU usage prev\"]\n",
    "concatenated_df[\"received_prev\"] = concatenated_df['Network received throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"received_diff\"] = concatenated_df['Network received throughput [KB/s]']- concatenated_df[\"received_prev\"]\n",
    "concatenated_df[\"transmitted_prev\"] = concatenated_df['Network transmitted throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"transmitted_diff\"] = concatenated_df['Network transmitted throughput [KB/s]']- concatenated_df[\"transmitted_prev\"]\n",
    "\n",
    "concatenated_df[\"write_prev\"] = concatenated_df['Disk write throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"write_diff\"] = concatenated_df['Disk write throughput [KB/s]']- concatenated_df[\"write_prev\"]\n",
    "concatenated_df[\"read_prev\"] = concatenated_df['Disk read throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"read_diff\"] = concatenated_df['Disk read throughput [KB/s]']- concatenated_df[\"read_prev\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values using forward propagating function from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = concatenated_df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new data frame: resampled & aggregated over each hour for all VMs.\n",
    "This allows to get a broader picture of the network's activity. \n",
    "Examine autocorrelations of hourly transmitted, received, and CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlydat = concatenated_df.resample('H').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hourly resampled means\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.plotting.autocorrelation_plot(hourlydat['CPU usage [MHZ]']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is CPU Capacity Ever Met?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overprovision = pd.DataFrame(hourlydat['CPU usage [MHZ]'])\n",
    "overprovision['CPU capacity provisioned'] = pd.DataFrame(hourlydat['CPU capacity provisioned [MHZ]'])\n",
    "#overprovision.to_csv(\"overprovision.csv\", sep = \",\")\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "overprovision.plot(figsize = (12,10),linewidth=2.5,fontsize=20)\n",
    "plt.title('Is CPU Capacity Ever Met?',fontsize=22)\n",
    "plt.ylabel((r'CPU [MHz]  $e^{7}$'), fontsize=20);\n",
    "plt.xlabel('Date', fontsize=20);\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks( fontsize = 15)\n",
    "plt.legend(loc=\"best\", fontsize =14)\n",
    "plt.ticklabel_format(axis = 'y', style = 'sci', scilimits = (1,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "    \n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.tick_params(labelsize=20)\n",
    "    plt.xlabel(\"Features\", fontsize = 20) \n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm  \n",
    "from statsmodels.tsa.stattools import acf  \n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(hourlydat['CPU usage [MHZ]'].copy())\n",
    "data.columns = [\"y\"]\n",
    "\n",
    "# Adding the lag of the target variable from 3 steps (hours) up to 24\n",
    "for i in range(3, 25):\n",
    "    data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "\n",
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for time-series cross-validation set 5 folds \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reserve 30% of data for testing\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"violet\", label=\"Prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values,\"green\", label=\"Actual Usage\", linewidth=2.0)\n",
    "    #plt.plot(capprv.values, color='red',label = \"Actual Provision\")\n",
    "\n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"Upper/Lower Bounds\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        plt.tick_params(labelsize=20)\n",
    "        plt.ylabel(\"CPU usage [MHZ]\", fontsize = 20)\n",
    "        plt.xlabel(\"Time\", fontsize = 20) \n",
    "        plt.yticks([2000000,6000000,12000000])# customized for graph readability\n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean Absolute Error: {0:.2f}%\".format(error),  fontsize = 20)\n",
    "    plt.legend(loc=\"best\", fontsize = 15)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelResults(model, plot_intervals=False)\n",
    "plotCoefficients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = model_path+'linear_regression.model'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaled Linear Regression MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where keys are unique categories\n",
    "    and values are means\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())\n",
    "\n",
    "def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n",
    "    \"\"\"\n",
    "        series: pd.DataFrame\n",
    "            dataframe with timeseries\n",
    "        lag_start: int\n",
    "            initial step back in time to slice target variable \n",
    "            example - lag_start = 1 means that the model \n",
    "                      will see yesterday's values to predict today\n",
    "        lag_end: int\n",
    "            final step back in time to slice target variable\n",
    "            example - lag_end = 4 means that the model \n",
    "                      will see up to 4 days back in time to predict today\n",
    "        test_size: float\n",
    "            size of the test dataset after train/test split as percentage of dataset\n",
    "        target_encoding: boolean\n",
    "            if True - add target averages to the dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    # copy of the initial dataset\n",
    "    data = pd.DataFrame(series.copy())\n",
    "    data.columns = [\"y\"]\n",
    "    \n",
    "    # lags of series\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "    \n",
    "    # datetime features\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    # other features\n",
    "    data['network received'] = hourlydat[['Network received throughput [KB/s]']]\n",
    "    data['network transmitted'] = hourlydat[['Network transmitted throughput [KB/s]']]\n",
    "    data['disk read'] = hourlydat[['Disk read throughput [KB/s]']]\n",
    "    data['disk write'] = hourlydat[['Disk write throughput [KB/s]']]\n",
    "    data['cpu diff'] = hourlydat[['CPU_diff']]\n",
    "    data['received_prev'] = hourlydat[['received_prev']]\n",
    "    data['core'] = hourlydat[['CPU cores']]\n",
    "\n",
    "    if target_encoding:\n",
    "        # calculate averages on train set only\n",
    "        test_index = int(len(data.dropna())*(1-test_size))\n",
    "        data['weekday_average'] = list(map(\n",
    "            code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday))\n",
    "        data[\"hour_average\"] = list(map(\n",
    "            code_mean(data[:test_index], 'hour', \"y\").get, data.hour))\n",
    "        \n",
    "        # drop encoded variables \n",
    "        data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "    \n",
    "    # train-test split\n",
    "    y = data.dropna().y\n",
    "    X = data.dropna().drop(['y'], axis=1)\n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    timeseries_train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve 30% of data for testing\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "prepareData(hourlydat[['CPU usage [MHZ]']], lag_start=3, lag_end=10, test_size=0.3, target_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, \n",
    "                 plot_intervals=False, plot_anomalies=False)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = model_path+'scaled_linear_regression.model'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
